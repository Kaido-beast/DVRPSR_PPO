{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d9399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "def export_train_test_stats(args, start_epoch, train_stats, test_stats):\n",
    "    fpath = os.path.join(args.output_dir, \"loss_gap.csv\")\n",
    "    with open(fpath, 'a') as f:\n",
    "        f.write((' '.join(\"{: >16}\" for _ in range(8)) + '\\n').format(\n",
    "            \"#EP\", \"#LOSS\", \"#PROB\", \"#VAL\", \"#C_Val\", \"#TEST_MU\", \"#TEST_STD\", \"#TEST_GAP\"\n",
    "        ))\n",
    "        for epoch, (train, test) in enumerate(zip_longest(train_stats, test_stats), start=start_epoch):\n",
    "            # train = train if train is not None else ['nan'] * 5\n",
    "            test = test if test is not None else [0.0, 0.0, 0.0]\n",
    "            f.write((\"{: >16d}\" + ' '.join(\"{: >16.3g}\" for _ in range(7)) + '\\n').format(\n",
    "                epoch, *train, *test))\n",
    "\n",
    "\n",
    "def save_checkpoint(args, epoch, model, optim, lr_scheduler=None):\n",
    "    checkout = {'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optim': optim.state_dict()}\n",
    "\n",
    "    if args.rate_decay is not None:\n",
    "        checkout['lr_scheduler'] = lr_scheduler.state_dict()\n",
    "    torch.save(checkout, os.path.join(args.output_dir, \"checkout_epoch{}.pth\".format(epoch+1)))\n",
    "\n",
    "\n",
    "def load_checkpoint(args, model, optim, baseline=None, lr_scheduler=None):\n",
    "    checkout = torch.load(args.resume_state)\n",
    "    model.load_state_dict(checkout['model'])\n",
    "    optim.load_state_dict(checkout['optim'])\n",
    "\n",
    "    if args.rate_decay is not None:\n",
    "        lr_scheduler.load_state_dict(checkout['lr_scheduler'])\n",
    "\n",
    "    return checkout['epoch']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911911cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class DVRPSR_Environment:\n",
    "    vehicle_feature = 8  # vehicle coordinates(x_i,y_i), veh_time_time_budget, total_travel_time, last_customer,\n",
    "    # next(destination) customer, last rewards, next rewards\n",
    "    customer_feature = 4\n",
    "\n",
    "    # TODO: change pending cost for rewards\n",
    "\n",
    "    def __init__(self, data, nodes=None, customer_mask=None, edges_attributes=None,\n",
    "                 pending_cost=1,\n",
    "                 dynamic_reward=0.2,\n",
    "                 budget_penalty=10):\n",
    "\n",
    "        self.vehicle_count = data.vehicle_count\n",
    "        self.vehicle_speed = data.vehicle_speed\n",
    "        self.vehicle_time_budget = data.vehicle_time_budget\n",
    "\n",
    "        self.nodes = data.nodes if nodes is None else nodes\n",
    "        self.edge_index = data.edges_index\n",
    "        self.edge_attributes = data.edges_attributes if edges_attributes is None else edges_attributes\n",
    "        self.init_customer_mask = data.customer_mask if customer_mask is None else customer_mask\n",
    "\n",
    "        self.minibatch, self.nodes_count, _ = self.nodes.size()\n",
    "        self.distance_matrix = self.edge_attributes.view((self.minibatch, self.nodes_count, self.nodes_count))\n",
    "        self.pending_cost = pending_cost\n",
    "        self.dynamic_reward = dynamic_reward\n",
    "        self.budget_penalty = budget_penalty\n",
    "\n",
    "    def _update_current_vehicles(self, dest, customer_index, tau=0):\n",
    "\n",
    "        # calculate travel time\n",
    "        # TODO: 1) in real world setting we need to calculate the distance of arc\n",
    "        # If nodes i and j are directly connected by a road segment (i, j) ∈ A, then t(i,j)=t_ij;\n",
    "        # otherwise, t(i,j)=t_ik1 +t_k1k2 +...+t_knj, where k1,...,kn ∈ V are the nodes along the\n",
    "        # shortest path from node i to node j.\n",
    "        #      2) calculate stating time for each vehicle $\\tau $, currently is set to zero\n",
    "\n",
    "        # update vehicle previous and next customer id\n",
    "        self.current_vehicle[:, :, 4] = self.current_vehicle[:, :, 5]\n",
    "        self.current_vehicle[:, :, 5] = customer_index\n",
    "\n",
    "        # get the distance from current vehicle to its next destination\n",
    "        dist = torch.zeros((self.minibatch, 1)).to(self.nodes.device)\n",
    "        for i in range(self.minibatch):\n",
    "            dist[i, 0] = self.distance_matrix[i][int(self.current_vehicle[i, :, 4])][int(self.current_vehicle[i, :, 5])]\n",
    "\n",
    "        # total travel time\n",
    "        tt = dist / self.vehicle_speed\n",
    "\n",
    "        # customers which are dynamicaly appeared\n",
    "        dyn_cust = (dest[:, :, 3] > 0).float()\n",
    "\n",
    "        # budget left while travelling to destination nodes\n",
    "        budget = tt + dest[:, :, 2]\n",
    "        # print(budget, tau, tt, dest[:,:,2])\n",
    "\n",
    "        # update vehicle features based on destination nodes\n",
    "        self.current_vehicle[:, :, :2] = dest[:, :, :2]\n",
    "        self.current_vehicle[:, :, 2] -= budget\n",
    "        self.current_vehicle[:, :, 3] += tt\n",
    "        self.current_vehicle[:, :, 6] = self.current_vehicle[:, :, 7]\n",
    "        self.current_vehicle[:, :, 7] = -dist\n",
    "\n",
    "        # update vehicles states\n",
    "        self.vehicles = self.vehicles.scatter(1,\n",
    "                                              self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                            self.vehicle_feature),\n",
    "                                              self.current_vehicle)\n",
    "\n",
    "        return dist, dyn_cust\n",
    "\n",
    "    def _done(self, customer_index):\n",
    "\n",
    "        self.vehicle_done.scatter_(1, self.current_vehicle_index, torch.logical_or((customer_index == 0),\n",
    "                                                                                   (self.current_vehicle[:, :,\n",
    "                                                                                    2] <= 0)))\n",
    "        # print(self.veh_done, cust_idx==0,self.cur_veh[:,:,2]<=0, (cust_idx==0) | (self.cur_veh[:,:,2]<=0))\n",
    "        self.done = bool(self.vehicle_done.all())\n",
    "\n",
    "    def _update_mask(self, customer_index):\n",
    "\n",
    "        self.new_customer = False\n",
    "        self.served.scatter_(1, customer_index, customer_index > 0)\n",
    "\n",
    "        # cost for a vehicle to go to customer and back to deport considering service duration\n",
    "        cost = torch.zeros((self.minibatch, self.nodes_count, 1)).to(self.nodes.device)\n",
    "        for i in range(self.minibatch):\n",
    "            for j in range(self.nodes_count):\n",
    "                dist_vehicle_customer_depot = self.distance_matrix[i][int(self.current_vehicle[i, :, 4])][j] + \\\n",
    "                                              self.distance_matrix[i][j][0]\n",
    "                cost[i, j] = dist_vehicle_customer_depot\n",
    "\n",
    "        cost = cost / self.vehicle_speed\n",
    "\n",
    "        cost += self.nodes[:, :, None, 2]\n",
    "\n",
    "        overtime_mask = self.current_vehicle[:, :, None, 2] - cost\n",
    "        overtime_mask = overtime_mask.squeeze(2).unsqueeze(1)\n",
    "        overtime = torch.zeros_like(self.mask).scatter_(1,\n",
    "                                                        self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                      self.nodes_count),\n",
    "                                                        overtime_mask < 0)\n",
    "\n",
    "        self.mask = self.mask | self.served[:, None, :] | overtime | self.vehicle_done[:, :, None]\n",
    "        self.mask[:, :, 0] = 0  # depot\n",
    "\n",
    "    # updating current vehicle to find the next available vehicle\n",
    "    def _update_next_vehicle(self, veh_index=None):\n",
    "\n",
    "        if veh_index is None:\n",
    "            avail = self.vehicles[:, :, 3].clone()\n",
    "            avail[self.vehicle_done] = float('inf')\n",
    "            self.current_vehicle_index = avail.argmin(1, keepdim=True)\n",
    "        else:\n",
    "            self.current_vehicle_index = veh_index\n",
    "\n",
    "        self.current_vehicle = self.vehicles.gather(1, self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                     self.vehicle_feature))\n",
    "        self.current_vehicle_mask = self.mask.gather(1, self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                      self.nodes_count))\n",
    "\n",
    "    def _update_dynamic_customers(self):\n",
    "\n",
    "        time = self.current_vehicle[:, :, 3].clone()\n",
    "\n",
    "        if self.init_customer_mask is None:\n",
    "            reveal_dyn_reqs = torch.logical_and((self.customer_mask), (self.nodes[:, :, 3] <= time))\n",
    "        else:\n",
    "            reveal_dyn_reqs = torch.logical_and((self.customer_mask ^ self.init_customer_mask),\n",
    "                                                (self.nodes[:, :, 3] <= time))\n",
    "\n",
    "        if reveal_dyn_reqs.any():\n",
    "            self.new_customer = True\n",
    "            self.customer_mask = self.customer_mask ^ reveal_dyn_reqs\n",
    "            self.mask = self.mask ^ reveal_dyn_reqs[:, None, :].expand(-1, self.vehicle_count, -1)\n",
    "            self.vehicle_done = torch.logical_and(self.vehicle_done, (reveal_dyn_reqs.any(1) ^ True).unsqueeze(1))\n",
    "            self.vehicles[:, :, 3] = torch.max(self.vehicles[:, :, 3], time)\n",
    "            self._update_next_vehicle()\n",
    "\n",
    "    def reset(self):\n",
    "        # reset vehicle (minibatch*veh_count*veh_feature)\n",
    "        self.vehicles = self.nodes.new_zeros((self.minibatch, self.vehicle_count, self.vehicle_feature))\n",
    "        self.vehicles[:, :, :2] = self.nodes[:, :1, :2]\n",
    "        self.vehicles[:, :, 2] = self.vehicle_time_budget\n",
    "\n",
    "        # reset vehicle done\n",
    "        self.vehicle_done = self.nodes.new_zeros((self.minibatch, self.vehicle_count), dtype=torch.bool)\n",
    "        self.done = False\n",
    "\n",
    "        # reset cust_mask\n",
    "        self.customer_mask = self.nodes[:, :, 3] > 0\n",
    "        if self.init_customer_mask is not None:\n",
    "            self.customer_mask = self.customer_mask | self.init_customer_mask\n",
    "\n",
    "        # reset new customers and served customer since now to zero (all false)\n",
    "        self.new_customer = True\n",
    "        self.served = torch.zeros_like(self.customer_mask)\n",
    "\n",
    "        # reset mask (minibatch*veh_count*nodes)\n",
    "        self.mask = self.customer_mask[:, None, :].repeat(1, self.vehicle_count, 1)\n",
    "\n",
    "        # reset current vehicle index, current vehicle, current vehicle mask\n",
    "        self.current_vehicle_index = self.nodes.new_zeros((self.minibatch, 1), dtype=torch.int64)\n",
    "\n",
    "        self.current_vehicle = self.vehicles.gather(1,\n",
    "                                                    self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                  self.vehicle_feature))\n",
    "        self.current_vehicle_mask = self.mask.gather(1,\n",
    "                                                     self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                   self.nodes_count))\n",
    "\n",
    "    def step(self, customer_index, veh_index=None):\n",
    "        dest = self.nodes.gather(1, customer_index[:, :, None].expand(-1, -1, self.customer_feature))\n",
    "        dist, dyn_cust = self._update_current_vehicles(dest, customer_index)\n",
    "\n",
    "        # cust = (dest[:, :, 3] >= 0).float()\n",
    "\n",
    "        self._done(customer_index)\n",
    "        self._update_mask(customer_index)\n",
    "        self._update_next_vehicle(veh_index)\n",
    "\n",
    "        # reward = -dist * (1 - dyn_cust*self.dynamic_reward)\n",
    "        reward = self.current_vehicle[:, :, 7] - self.current_vehicle[:, :, 6] + self.dynamic_reward * dyn_cust\n",
    "        pending_static_customers = torch.logical_and((self.served ^ True),\n",
    "                                                     (self.nodes[:, :, 3] == 0)).float().sum(-1, keepdim=True) - 1\n",
    "\n",
    "        reward -= self.pending_cost * pending_static_customers\n",
    "\n",
    "        if self.done:\n",
    "\n",
    "            if self.init_customer_mask is not None:\n",
    "                self.served += self.init_customer_mask\n",
    "            # penalty for pending customers\n",
    "            pending_customers = torch.logical_and((self.served ^ True),\n",
    "                                                  (self.nodes[:, :, 3] >= 0)).float().sum(-1, keepdim=True) - 1\n",
    "\n",
    "            # TODO: penalty for having unused time budget as well not serving customers\n",
    "            reward -= self.dynamic_reward * pending_customers\n",
    "\n",
    "        self._update_dynamic_customers()\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def state_dict(self, dest_dict=None):\n",
    "        if dest_dict is None:\n",
    "            dest_dict = {'vehicles': self.vehicles,\n",
    "                         'vehicle_done': self.vehicle_done,\n",
    "                         'served': self.served,\n",
    "                         'mask': self.mask,\n",
    "                         'current_vehicle_index': self.current_vehicle_index}\n",
    "\n",
    "        else:\n",
    "            dest_dict[\"vehicles\"].copy_(self.vehicles)\n",
    "            dest_dict[\"vehicle_done\"].copy_(self.vehicle_done)\n",
    "            dest_dict[\"served\"].copy_(self.served)\n",
    "            dest_dict[\"mask\"].copy_(self.mask)\n",
    "            dest_dict[\"current_vehicle_index\"].copy_(self.current_vehicle_index)\n",
    "\n",
    "        return dest_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.vehicles.copy_(state_dict[\"vehicles\"])\n",
    "        self.vehicle_done.copy_(state_dict[\"vehicle_done\"])\n",
    "        self.served.copy_(state_dict[\"served\"])\n",
    "        self.mask.copy_(state_dict[\"mask\"])\n",
    "        self.current_vehicle_index.copy_(state_dict[\"current_vehicle_index\"])\n",
    "\n",
    "        self.current_vehicle = self.vehicles.gather(1,\n",
    "                                                    self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                  self.vehicle_feature))\n",
    "        self.current_vehicle_mask = self.mask.gather(1, self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                      self.customer_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc7944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, query_size, key_size=None, value_size=None, edge_dim_size=None, bias=False):\n",
    "\n",
    "        super(GraphMultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.query_size = query_size\n",
    "\n",
    "        self.key_size = self.query_size if key_size is None else key_size\n",
    "        self.value_size = self.key_size if value_size is None else value_size\n",
    "        self.edge_dim_size = self.query_size // 2 if edge_dim_size is None else edge_dim_size\n",
    "\n",
    "        self.scaling_factor = self.key_size ** -0.5\n",
    "\n",
    "        self.keys_per_head = self.key_size // self.num_head\n",
    "        self.values_per_head = self.value_size // self.num_head\n",
    "        self.edge_size_per_head = self.edge_dim_size\n",
    "\n",
    "        self.edge_embedding = nn.Linear(self.edge_dim_size, self.edge_size_per_head, bias=bias)\n",
    "        self.query_embedding = nn.Linear(self.query_size, self.num_head * self.keys_per_head, bias=bias)\n",
    "        self.key_embedding = nn.Linear(self.key_size, self.num_head * self.keys_per_head, bias=bias)\n",
    "        self.value_embedding = nn.Linear(self.value_size, self.num_head * self.values_per_head, bias=bias)\n",
    "        self.recombine = nn.Linear(self.num_head * self.values_per_head, self.value_size, bias=bias)\n",
    "\n",
    "        self.K_project_pre = None\n",
    "        self.V_project_pre = None\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # TODO: add xavier initialziation as well\n",
    "\n",
    "        nn.init.uniform_(self.query_embedding.weight, -self.scaling_factor, self.scaling_factor)\n",
    "        nn.init.uniform_(self.key_embedding.weight, -self.scaling_factor, self.scaling_factor)\n",
    "        inv_sq_dv = self.value_size ** -0.5\n",
    "        nn.init.uniform_(self.value_embedding.weight, -inv_sq_dv, inv_sq_dv)\n",
    "\n",
    "    def precompute(self, keys, values=None):\n",
    "\n",
    "        values = keys if values is None else values\n",
    "\n",
    "        size_KV = keys.size(-2)\n",
    "\n",
    "        self.K_project_pre = self.key_embedding(keys).view(\n",
    "            -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "\n",
    "        self.V_project_pre = self.value_embedding(values).view(\n",
    "            -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, queries, keys=None, values=None, edge_attributes=None, mask=None, edge_mask=None):\n",
    "\n",
    "        *batch_size, size_Q, _ = queries.size()\n",
    "\n",
    "        # get queries projection\n",
    "        Q_project = self.query_embedding(queries).view(\n",
    "            -1, size_Q, self.num_head, self.keys_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get keys projection\n",
    "        if keys is None:\n",
    "            if self.K_project_pre is None:\n",
    "                size_KV = size_Q\n",
    "                K_project = self.key_embedding(queries).view(\n",
    "                    -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "            else:\n",
    "                size_KV = self.K_project_pre.size(-1)\n",
    "                K_project = self.K_project_pre\n",
    "        else:\n",
    "            size_KV = keys.size(-2)\n",
    "            K_project = self.key_embedding(keys).view(\n",
    "                -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "\n",
    "        # get values projection\n",
    "        if values is None:\n",
    "            if self.V_project_pre is None:\n",
    "                V_project = self.value_embedding(queries).view(\n",
    "                    -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "            else:\n",
    "                V_project = self.V_project_pre\n",
    "        else:\n",
    "            V_project = self.value_embedding(values).view(\n",
    "                -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # calculate the compability\n",
    "        attention = Q_project.matmul(K_project)\n",
    "        attention *= self.scaling_factor\n",
    "\n",
    "        # if edge attributes are required\n",
    "        if edge_attributes is not None:\n",
    "            # TODO: edge mask (is it required)\n",
    "            edge_project = self.edge_embedding(edge_attributes).view(\n",
    "                -1, size_Q, size_Q, self.edge_size_per_head)\n",
    "\n",
    "            # get enhanced attention inclusing edge attributes\n",
    "            attention_expanded = attention.unsqueeze(-1).expand(-1, -1, -1, -1, self.edge_size_per_head)\n",
    "\n",
    "            # Expand edge attributes to match the number of attention heads\n",
    "            edge_project_expanded = edge_project.unsqueeze(1).expand(-1, attention.size(1), -1, -1, -1)\n",
    "\n",
    "            attention = attention_expanded * edge_project_expanded\n",
    "            attention = attention.mean(-1)\n",
    "\n",
    "            # print(attention.size())\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            if mask.numel() * self.num_head == attention.numel():\n",
    "                m = mask.view(-1, 1, size_Q, size_KV).expand_as(attention)\n",
    "            else:\n",
    "                m = mask.view(-1, 1, 1, size_KV).expand_as(attention)\n",
    "\n",
    "            attention[m.bool()] = -float('inf')\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = attention.matmul(V_project).permute(0, 2, 1, 3).contiguous().view(\n",
    "            *batch_size, size_Q, self.num_head * self.values_per_head)\n",
    "\n",
    "        output = self.recombine(attention)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9f7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, query_size, key_size=None, value_size=None, edge_dim_size=None, bias=False):\n",
    "\n",
    "        super(GraphMultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.query_size = query_size\n",
    "\n",
    "        self.key_size = self.query_size if key_size is None else key_size\n",
    "        self.value_size = self.key_size if value_size is None else value_size\n",
    "        self.edge_dim_size = self.query_size // 2 if edge_dim_size is None else edge_dim_size\n",
    "\n",
    "        self.scaling_factor = self.key_size ** -0.5\n",
    "\n",
    "        self.keys_per_head = self.key_size // self.num_head\n",
    "        self.values_per_head = self.value_size // self.num_head\n",
    "        self.edge_size_per_head = self.edge_dim_size\n",
    "\n",
    "        self.edge_embedding = nn.Linear(self.edge_dim_size, self.edge_size_per_head, bias=bias)\n",
    "        self.query_embedding = nn.Linear(self.query_size, self.num_head * self.keys_per_head, bias=bias)\n",
    "        self.key_embedding = nn.Linear(self.key_size, self.num_head * self.keys_per_head, bias=bias)\n",
    "        self.value_embedding = nn.Linear(self.value_size, self.num_head * self.values_per_head, bias=bias)\n",
    "        self.recombine = nn.Linear(self.num_head * self.values_per_head, self.value_size, bias=bias)\n",
    "\n",
    "        self.K_project_pre = None\n",
    "        self.V_project_pre = None\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # TODO: add xavier initialziation as well\n",
    "\n",
    "        nn.init.uniform_(self.query_embedding.weight, -self.scaling_factor, self.scaling_factor)\n",
    "        nn.init.uniform_(self.key_embedding.weight, -self.scaling_factor, self.scaling_factor)\n",
    "        inv_sq_dv = self.value_size ** -0.5\n",
    "        nn.init.uniform_(self.value_embedding.weight, -inv_sq_dv, inv_sq_dv)\n",
    "\n",
    "    def precompute(self, keys, values=None):\n",
    "\n",
    "        values = keys if values is None else values\n",
    "\n",
    "        size_KV = keys.size(-2)\n",
    "\n",
    "        self.K_project_pre = self.key_embedding(keys).view(\n",
    "            -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "\n",
    "        self.V_project_pre = self.value_embedding(values).view(\n",
    "            -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, queries, keys=None, values=None, edge_attributes=None, mask=None, edge_mask=None):\n",
    "\n",
    "        *batch_size, size_Q, _ = queries.size()\n",
    "\n",
    "        # get queries projection\n",
    "        Q_project = self.query_embedding(queries).view(\n",
    "            -1, size_Q, self.num_head, self.keys_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get keys projection\n",
    "        if keys is None:\n",
    "            if self.K_project_pre is None:\n",
    "                size_KV = size_Q\n",
    "                K_project = self.key_embedding(queries).view(\n",
    "                    -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "            else:\n",
    "                size_KV = self.K_project_pre.size(-1)\n",
    "                K_project = self.K_project_pre\n",
    "        else:\n",
    "            size_KV = keys.size(-2)\n",
    "            K_project = self.key_embedding(keys).view(\n",
    "                -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "\n",
    "        # get values projection\n",
    "        if values is None:\n",
    "            if self.V_project_pre is None:\n",
    "                V_project = self.value_embedding(queries).view(\n",
    "                    -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "            else:\n",
    "                V_project = self.V_project_pre\n",
    "        else:\n",
    "            V_project = self.value_embedding(values).view(\n",
    "                -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # calculate the compability\n",
    "        attention = Q_project.matmul(K_project)\n",
    "        attention *= self.scaling_factor\n",
    "\n",
    "        # if edge attributes are required\n",
    "        if edge_attributes is not None:\n",
    "            # TODO: edge mask (is it required)\n",
    "            edge_project = self.edge_embedding(edge_attributes).view(\n",
    "                -1, size_Q, size_Q, self.edge_size_per_head)\n",
    "\n",
    "            # get enhanced attention inclusing edge attributes\n",
    "            attention_expanded = attention.unsqueeze(-1).expand(-1, -1, -1, -1, self.edge_size_per_head)\n",
    "\n",
    "            # Expand edge attributes to match the number of attention heads\n",
    "            edge_project_expanded = edge_project.unsqueeze(1).expand(-1, attention.size(1), -1, -1, -1)\n",
    "\n",
    "            attention = attention_expanded * edge_project_expanded\n",
    "            attention = attention.mean(-1)\n",
    "\n",
    "            # print(attention.size())\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            if mask.numel() * self.num_head == attention.numel():\n",
    "                m = mask.view(-1, 1, size_Q, size_KV).expand_as(attention)\n",
    "            else:\n",
    "                m = mask.view(-1, 1, 1, size_KV).expand_as(attention)\n",
    "\n",
    "            attention[m.bool()] = -float('inf')\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = attention.matmul(V_project).permute(0, 2, 1, 3).contiguous().view(\n",
    "            *batch_size, size_Q, self.num_head * self.values_per_head)\n",
    "\n",
    "        output = self.recombine(attention)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9822abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nets import GraphMultiHeadAttention\n",
    "\n",
    "class GraphEncoderlayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, model_size, ff_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = GraphMultiHeadAttention(num_head, query_size=model_size)\n",
    "        self.BN1 = nn.BatchNorm1d(model_size)\n",
    "        self.FFN_layer1 = nn.Linear(model_size, ff_size)\n",
    "\n",
    "        self.FFN_layer2 = nn.Linear(ff_size, model_size)\n",
    "        self.BN2 = nn.BatchNorm1d(model_size)\n",
    "\n",
    "    def forward(self, h, e=None, mask=None):\n",
    "        h_attn = self.attention(h, edge_attributes=e, mask=mask)\n",
    "        h_bn = self.BN1((h_attn + h).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        h_layer1 = F.relu(self.FFN_layer1(h_bn))\n",
    "        h_layer2 = self.FFN_layer2(h_layer1)\n",
    "\n",
    "        h_out = self.BN2((h_bn + h_layer2).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        if mask is not None:\n",
    "            h_out[mask] = 0\n",
    "\n",
    "        return h_out\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_head, model_size, ff_szie):\n",
    "        super().__init__()\n",
    "\n",
    "        for l in range(encoder_layer):\n",
    "            self.add_module(str(l), GraphEncoderlayer(num_head, model_size, ff_szie))\n",
    "\n",
    "    def forward(self, h_in, e_in=None, mask=None):\n",
    "\n",
    "        h = h_in\n",
    "        e = e_in\n",
    "\n",
    "        for child in self.children():\n",
    "            h = child(h, e, mask=mask)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cffa306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from nets import GraphMultiHeadAttention\n",
    "from nets.Encoder import GraphEncoder\n",
    "\n",
    "class GraphAttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, customer_feature, vehicle_feature, model_size=128, encoder_layer=3,\n",
    "                 num_head=8, ff_size=128, tanh_xplor=10, edge_embedding_dim=64, greedy=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # get models parameters for encoding-decoding\n",
    "        self.model_size = model_size\n",
    "        self.scaling_factor = self.model_size ** 0.5\n",
    "        self.tanh_xplor = tanh_xplor\n",
    "        self.greedy = greedy\n",
    "\n",
    "        # Initialize encoder and embeddings\n",
    "        self.customer_encoder = GraphEncoder(encoder_layer=3, num_head=8, model_size=128, ff_szie=512)\n",
    "        self.customer_embedding = nn.Linear(customer_feature, model_size)\n",
    "        self.depot_embedding = nn.Linear(customer_feature, model_size)\n",
    "\n",
    "        # initialize edge embedding\n",
    "        self.edge_embedding = nn.Linear(1, edge_embedding_dim)\n",
    "\n",
    "        # Initialize vehicle embedding and encoding\n",
    "        # self.vehicle_embedding = nn.Linear(vehicle_feature, ff_size, bias=False)\n",
    "\n",
    "        self.fleet_attention = GraphMultiHeadAttention(num_head, vehicle_feature, model_size)\n",
    "\n",
    "        self.vehicle_attention = GraphMultiHeadAttention(num_head, model_size)\n",
    "\n",
    "        # customer projection\n",
    "        self.customer_projection = nn.Linear(self.model_size, self.model_size)  # TODO: MLP instaed of nn.Linear\n",
    "\n",
    "    def encode_customers(self, env, customer_mask=None):\n",
    "\n",
    "        customer_emb = torch.cat((self.depot_embedding(env.nodes[:, :1, :]),\n",
    "                                  self.customer_embedding(env.nodes[:, 1:, :])), dim=1)\n",
    "        if customer_mask is not None:\n",
    "            customer_emb[customer_mask] = 0\n",
    "\n",
    "        edge_emb = self.edge_embedding(env.edge_attributes)\n",
    "\n",
    "        self.customer_encoding = self.customer_encoder(customer_emb, edge_emb, mask=customer_mask)\n",
    "\n",
    "        self.fleet_attention.precompute(self.customer_encoding)\n",
    "\n",
    "        self.customer_representation = self.customer_projection(self.customer_encoding)\n",
    "        if customer_mask is not None:\n",
    "            self.customer_representation[customer_mask] = 0\n",
    "\n",
    "    def vehicle_representation(self, vehicles, vehicle_index, vehicle_mask=None):\n",
    "\n",
    "        # vehicles_embedding = self.vehicle_embedding(vehicles)\n",
    "\n",
    "        # print(vehicles_embedding.size(), self.customer_representation.size())\n",
    "\n",
    "        fleet_representation = self.fleet_attention(vehicles, mask=vehicle_mask)\n",
    "\n",
    "        #         print(fleet_representation.size())\n",
    "\n",
    "        vehicle_query = fleet_representation.gather(0, vehicle_index.unsqueeze(2).expand(-1, -1, self.model_size))\n",
    "\n",
    "        self._vehicle_representation = self.vehicle_attention(vehicle_query,\n",
    "                                                              fleet_representation,\n",
    "                                                              fleet_representation)\n",
    "\n",
    "        return self._vehicle_representation\n",
    "\n",
    "    def score_customers(self, vehicle_representation):\n",
    "\n",
    "        # print(vehicle_representation.size(), self.customer_representation.size())\n",
    "        compact = torch.bmm(vehicle_representation,\n",
    "                            self.customer_representation.transpose(2, 1))\n",
    "        compact *= self.scaling_factor\n",
    "\n",
    "        if self.tanh_xplor is not None:\n",
    "            compact = self.tanh_xplor * compact.tanh()\n",
    "\n",
    "        return compact\n",
    "\n",
    "    def get_prop(self, compact, vehicle_mask=None):\n",
    "\n",
    "        compact = compact\n",
    "\n",
    "        compact[vehicle_mask] = -float('inf')\n",
    "        compact = F.softmax(compact, dim=-1)\n",
    "        return compact\n",
    "\n",
    "    def step(self, env, old_action=None):\n",
    "\n",
    "        _vehicle_representation = self.vehicle_representation(env.vehicles,\n",
    "                                                              env.current_vehicle_index,\n",
    "                                                              env.current_vehicle_mask)\n",
    "\n",
    "        compact = self.score_customers(_vehicle_representation)\n",
    "        prop = self.get_prop(compact, env.current_vehicle_mask)\n",
    "        # print(compact.size())\n",
    "\n",
    "        # step actions based on model act or evalaute\n",
    "        if old_action is not None:\n",
    "\n",
    "            # get entropy\n",
    "            dist = Categorical(prop)\n",
    "            old_actions_logp = dist.log_prob(old_action[:, 1].unsqueeze(-1))\n",
    "            entropy = dist.entropy()\n",
    "\n",
    "            is_done = float(env.done)\n",
    "\n",
    "            entropy = entropy * (1. - is_done)\n",
    "            old_actions_logp = old_actions_logp * (1. - is_done)\n",
    "            return old_action[:, 1].unsqueeze(-1), entropy, old_actions_logp\n",
    "\n",
    "\n",
    "        else:\n",
    "            dist = Categorical(prop)\n",
    "\n",
    "            if self.greedy:\n",
    "                _, customer_index = p.max(dim=-1)\n",
    "            else:\n",
    "                customer_index = dist.sample()\n",
    "\n",
    "            is_done = float(env.done)\n",
    "\n",
    "            logp = dist.log_prob(customer_index)\n",
    "            logp = logp * (1. - is_done)\n",
    "\n",
    "            return customer_index, logp\n",
    "\n",
    "    def forward(self, env, old_actions=None, is_update=False):\n",
    "\n",
    "        if is_update:\n",
    "            env.reset()\n",
    "            entropys, old_actions_logps = [], []\n",
    "\n",
    "            steps = old_actions.size(0)\n",
    "\n",
    "            for i in range(steps):\n",
    "                if env.new_customer:\n",
    "                    self.encode_customers(env, env.customer_mask)\n",
    "\n",
    "                if i < steps - 1:\n",
    "                    old_action = old_actions[i, :, :]\n",
    "                    next_action = old_actions[i + 1, :, :]\n",
    "                else:\n",
    "                    # this would be the last action which the agent takes and envrionment is done\n",
    "                    old_action = old_actions[i, :, :]\n",
    "                    next_action = old_actions[i, :, :]\n",
    "\n",
    "                next_vehicle_index = next_action[:, 0].unsqueeze(-1)\n",
    "                # print(next_vehicle_index)\n",
    "\n",
    "                customer_index, entropy, logp = self.step(env, old_action)\n",
    "\n",
    "                env.step(customer_index, next_vehicle_index)\n",
    "\n",
    "                old_actions_logps.append(logp)\n",
    "                entropys.append(entropy)\n",
    "\n",
    "            entropys = torch.cat(entropys, dim=1)\n",
    "            num_e = entropys.ne(0).float().sum(1)\n",
    "            entropy = entropys.sum(1) / num_e\n",
    "\n",
    "            old_actions_logps = torch.cat(old_actions_logps, dim=1)\n",
    "            old_actions_logps = old_actions_logps.sum(1)\n",
    "\n",
    "            return entropy, old_actions_logps, 0\n",
    "\n",
    "        else:\n",
    "            env.reset()\n",
    "            actions, logps, rewards = [], [], []\n",
    "\n",
    "            while not env.done:\n",
    "                if env.new_customer:\n",
    "                    self.encode_customers(env, env.customer_mask)\n",
    "\n",
    "                customer_index, logp = self.step(env)\n",
    "                actions.append((env.current_vehicle_index, customer_index))\n",
    "                logps.append(logp)\n",
    "                rewards.append(env.step(customer_index))\n",
    "\n",
    "            # actions = torch.cat(actions, dim=1)\n",
    "            logps = torch.cat(logps, dim=1)\n",
    "            logp = logps.sum(dim=1)\n",
    "\n",
    "            rewards = torch.cat(rewards, dim=1)\n",
    "            rewards = rewards.sum(dim=1)\n",
    "\n",
    "            return actions, logp, rewards\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19584eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    # critic will take environment as imput and ouput the values for loss function\n",
    "    # which is basically the estimation of complexity of actions\n",
    "\n",
    "    def __init__(self, model, customers_count, ff_size=512):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.ff_layer1 = nn.Linear(customers_count, ff_size)\n",
    "        self.ff_layer2 = nn.Linear(ff_size, customers_count)\n",
    "\n",
    "    def eval_step(self, env, compatibility, customer_index):\n",
    "        compact = compatibility.clone()\n",
    "        compact[env.current_vehicle_mask] = 0\n",
    "\n",
    "        value = self.ff_layer1(compact)\n",
    "        value = F.relu(value)\n",
    "        value = self.ff_layer2(value)\n",
    "\n",
    "        val = value.gather(2, customer_index.unsqueeze(1)).expand(-1, 1, -1)\n",
    "        return val.squeeze(1)\n",
    "\n",
    "    def __call__(self, env):\n",
    "        self.model.encode_customers(env)\n",
    "        env.reset()\n",
    "\n",
    "        values = []\n",
    "\n",
    "        while not env.done:\n",
    "            _vehicle_presentation = self.model.vehicle_representation(env.vehicles,\n",
    "                                                                      env.current_vehicle_index,\n",
    "                                                                      env.current_vehicle_mask)\n",
    "            compatibility = self.model.score_customers(_vehicle_presentation)\n",
    "            prop = self.model.get_prop(compatibility, env.current_vehicle_mask)\n",
    "            dist = Categorical(prop)\n",
    "            customer_index = dist.sample()\n",
    "\n",
    "            values.append(self.eval_step(env, compatibility, customer_index))\n",
    "\n",
    "            return values[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7751eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nets import GraphAttentionModel\n",
    "from agents.Critic import Critic\n",
    "\n",
    "\n",
    "class Actor_Critic(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 customer_feature,\n",
    "                 vehicle_feature,\n",
    "                 customers_count,\n",
    "                 model_size=128,\n",
    "                 encoder_layer=3,\n",
    "                 num_head=8,\n",
    "                 ff_size_actor=128,\n",
    "                 ff_size_critic=512,\n",
    "                 tanh_xplor=10,\n",
    "                 edge_embedding_dim=64,\n",
    "                 greedy=False):\n",
    "        super(Actor_Critic, self).__init__()\n",
    "\n",
    "        model = GraphAttentionModel(customer_feature, vehicle_feature, model_size, encoder_layer,\n",
    "                                    num_head, ff_size_actor, tanh_xplor, edge_embedding_dim, greedy)\n",
    "        self.actor = model\n",
    "\n",
    "        self.critic = Critic(model, customers_count, ff_size_critic)\n",
    "\n",
    "    def act(self, env, old_actions=None, is_update=False):\n",
    "        actions, logps, rewards = self.actor(env)\n",
    "        return actions, logps, rewards\n",
    "\n",
    "    def evaluate(self, env, old_actions, is_update):\n",
    "        entropys, old_logps, _ = self.actor(env, old_actions, is_update)\n",
    "        values = self.critic(env)\n",
    "        return entropys, old_logps, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f065314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "        self.edge_attributes = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.nodes.clear()\n",
    "        self.edge_attributes.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.log_probs.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d12b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import time\n",
    "\n",
    "from agents.Actor_Critic import Actor_Critic\n",
    "from problems import DVRPSR_Environment\n",
    "\n",
    "\n",
    "class AgentPPO:\n",
    "\n",
    "    def __init__(self,\n",
    "                 customer_feature,\n",
    "                 vehicle_feature,\n",
    "                 customers_count,\n",
    "                 model_size=128,\n",
    "                 encoder_layer=3,\n",
    "                 num_head=8,\n",
    "                 ff_size_actor=128,\n",
    "                 ff_size_critic=512,\n",
    "                 tanh_xplor=10,\n",
    "                 edge_embedding_dim=64,\n",
    "                 greedy=False,\n",
    "                 learning_rate=3e-4,\n",
    "                 ppo_epoch=3,\n",
    "                 batch_size=256,\n",
    "                 entropy_value=0.2,\n",
    "                 epsilon_clip=0.2,\n",
    "                 max_grad_norm = 2):\n",
    "\n",
    "        self.policy = Actor_Critic(customer_feature, vehicle_feature, customers_count, model_size,\n",
    "                                   encoder_layer, num_head, ff_size_actor, ff_size_critic,\n",
    "                                   tanh_xplor, edge_embedding_dim, greedy)\n",
    "\n",
    "        self.old_policy = Actor_Critic(customer_feature, vehicle_feature, customers_count, model_size,\n",
    "                                       encoder_layer, num_head, ff_size_actor, ff_size_critic,\n",
    "                                       tanh_xplor, edge_embedding_dim, greedy)\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # ppo update parameters\n",
    "        # self.learning_rate = learning_rate\n",
    "        self.ppo_epoch = ppo_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_value = entropy_value\n",
    "        self.epsilon_clip = epsilon_clip\n",
    "        self.batch_index = 1\n",
    "\n",
    "        # initialize the Adam optimizer\n",
    "        #self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "\n",
    "        # actor-critic parameters\n",
    "        self.customer_feature = customer_feature\n",
    "        self.vehicle_feature = vehicle_feature\n",
    "        self.customers_count = customers_count\n",
    "        self.model_size = model_size\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.num_head = num_head\n",
    "        self.ff_size_actor = ff_size_actor\n",
    "        self.ff_size_critic = ff_size_critic\n",
    "        self.tanh_xplor = tanh_xplor\n",
    "        self.edge_embedding_dim = edge_embedding_dim\n",
    "        self.greedy = greedy\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.times, self.losses, self.rewards, self.critic_rewards = [], [], [], []\n",
    "\n",
    "    def advantage_normalization(self, advantage):\n",
    "\n",
    "        std = advantage.std()\n",
    "\n",
    "        assert std != 0. and not torch.isnan(std), 'Need nonzero std'\n",
    "\n",
    "        norm_advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        return norm_advantage\n",
    "\n",
    "    def pad_actions(self, actions):\n",
    "        max_len = max([a.size(0) for a in actions])\n",
    "        padded_actions = []\n",
    "        for a in actions:\n",
    "            pad_length = max_len - a.size(0)\n",
    "            padded_a = F.pad(a, (0, 0, 0, pad_length))\n",
    "            padded_actions.append(padded_a)\n",
    "        return torch.stack(padded_actions), max_len\n",
    "\n",
    "    def update(self, memory, epoch, data=None, env=None, optim=None, lr_scheduler=None, device=None):\n",
    "\n",
    "        old_nodes = torch.stack(memory.nodes)\n",
    "        old_edge_attributes = torch.stack(memory.edge_attributes)\n",
    "        old_rewards = torch.stack(memory.rewards).unsqueeze(-1)\n",
    "        old_log_probs = torch.stack(memory.log_probs).unsqueeze(-1)\n",
    "\n",
    "        # preprocessing on old actions\n",
    "        padded_actions, max_length = self.pad_actions(memory.actions)\n",
    "\n",
    "        # create update data for PPO\n",
    "        datas = []\n",
    "\n",
    "        # print(memory.actions.size())\n",
    "\n",
    "        for i in range(old_nodes.size(0)):\n",
    "            data_to_load = Data(nodes=old_nodes[i],\n",
    "                                edge_attributes=old_edge_attributes[i],\n",
    "                                actions=padded_actions[i],\n",
    "                                rewards=old_rewards[i],\n",
    "                                log_probs=old_log_probs[i])\n",
    "\n",
    "            datas.append(data_to_load)\n",
    "        # print(datas[0], self.batch_size)\n",
    "\n",
    "        self.policy.to(device)\n",
    "\n",
    "        data_loader = DataLoader(datas, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # scheduler = LambdaLR(self.optimizer, lr_lambda=lambda f: 0.96 ** epoch)\n",
    "        value_buffer = 0\n",
    "\n",
    "        env = env if env is not None else DVRPSR_Environment\n",
    "\n",
    "        for i in range(self.ppo_epoch):\n",
    "\n",
    "            self.policy.train()\n",
    "            epoch_start = time.time()\n",
    "            start = epoch_start\n",
    "\n",
    "            self.times, self.losses, self.rewards, self.critic_rewards = [], [], [], []\n",
    "\n",
    "            for batch_index, minibatch_data in enumerate(data_loader):\n",
    "\n",
    "                self.batch_index += 1\n",
    "                minibatch_data.to(device)\n",
    "\n",
    "                if data.customer_mask is None:\n",
    "                    nodes = minibatch_data.nodes.to(device)\n",
    "                    customer_mask = None\n",
    "                    edge_attributes = minibatch_data.edge_attributes.to(device)\n",
    "\n",
    "                nodes = nodes.view(self.batch_size, self.customers_count, self.customer_feature)\n",
    "                edge_attributes = edge_attributes.view(self.batch_size, self.customers_count * self.customers_count, 1)\n",
    "\n",
    "                old_actions_for_env = minibatch_data.actions.view(self.batch_size, max_length, 2).permute(1, 0, 2).to(device)\n",
    "                #print(old_actions_for_env)\n",
    "\n",
    "                dyna_env = env(data, nodes, customer_mask, edge_attributes)\n",
    "\n",
    "                entropy, log_probs, values = self.policy.evaluate(dyna_env, old_actions_for_env, True)\n",
    "\n",
    "                # normalize the rewards and get the MSE loss with critics values\n",
    "                R = minibatch_data.rewards\n",
    "                R_norm = self.advantage_normalization(R)\n",
    "\n",
    "                mse_loss = self.MSE_loss(R_norm, values.squeeze(-1))\n",
    "\n",
    "                # PPO ration (r(0)_t)\n",
    "                ratio = torch.exp(log_probs - minibatch_data.log_probs)\n",
    "\n",
    "                # PPO advantage\n",
    "                advantage = R_norm - values.detach()\n",
    "\n",
    "                # PPO overall loss function\n",
    "                actor_loss1 = ratio * advantage\n",
    "                actor_loss2 = torch.clamp(ratio, 1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantage\n",
    "\n",
    "                actor_loss = torch.min(actor_loss1, actor_loss2)\n",
    "\n",
    "                # total loss\n",
    "                loss = actor_loss + 0.5 * mse_loss - self.entropy_value * entropy\n",
    "\n",
    "                # optimizer and backpropogation\n",
    "                #self.optimizer.zero_grad()\n",
    "                optim.zero_grad()\n",
    "                loss.mean().backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                #self.optimizer.step()\n",
    "                optim.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                self.rewards.append(torch.mean(R_norm.detach()).item())\n",
    "                self.losses.append(torch.mean(loss.detach()).item())\n",
    "                self.critic_rewards.append(torch.mean(values.detach()).item())\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        return self.rewards, self.losses, self.critic_rewards\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     raise Exception('Cannot be called from main')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea6451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "def write_config_file(args, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(vars(args), f, indent=4)\n",
    "\n",
    "\n",
    "def ParseArguments(argv=None):\n",
    "    parser = ArgumentParser()\n",
    "    parser = argparse.ArgumentParser(description=\"Reinforcement Learning for Dynamic VRP with Stochastic Requests\")\n",
    "\n",
    "    parser.add_argument(\"--config-file\", \"-f\", type=str, default=None,\n",
    "                        help=\"configuration file\")\n",
    "    parser.add_argument(\"--verbose\", \"-v\", action='store_true', default=True,\n",
    "                        help=\"Showing information while processing\")\n",
    "    parser.add_argument(\"--gpu\", action='store_true', default=True,\n",
    "                        help=\"Use GPU to run the model\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"seed to regenerate same result\")\n",
    "\n",
    "    ### Data related arguments\n",
    "\n",
    "    parser.add_argument_group(\"Data Generation for DVRPSR\")\n",
    "    parser.add_argument(\"--problem\", \"-p\", type=str, default='DVRPSR',\n",
    "                        help=\"problem to solve is DVRPSR\")\n",
    "    parser.add_argument(\"--vehicle-count\", \"-m\", type=int, default=2,\n",
    "                        help='number of vehicles for DVRPSR')\n",
    "    parser.add_argument(\"--vehicle-speed\", type=int, default=1.2,\n",
    "                        help='speed of vehicle for DVRPSR')\n",
    "    parser.add_argument(\"--Lambda\", type=float, default=0.025,\n",
    "                        help='Requests rate per minute')\n",
    "    parser.add_argument(\"--dod\", type=float, default=0.5,\n",
    "                        help=\"Degree of dynamism\")\n",
    "    parser.add_argument(\"--horizon\", type=int, default=400,\n",
    "                        help='Working time for DVRPSR in minutes')\n",
    "    parser.add_argument(\"--customers_count\", type=int, default=None,\n",
    "                        help='Working time for DVRPSR in minutes')\n",
    "    parser.add_argument(\"--fDmean\", type=int, default=10,\n",
    "                        help=\"mean value for service duration of customers\")\n",
    "    parser.add_argument(\"--fDstd\", type=float, default=2.5,\n",
    "                        help=\"standard deviation for service duration of customers\")\n",
    "    parser.add_argument(\"--euclidean\", action = 'store_true', default=True,\n",
    "                        help=\"Wheather to use Euclidean distance or City street network for distance calculation\")\n",
    "\n",
    "    ### Environment related arguments\n",
    "    parser.add_argument_group(\" Environment for DVRPSR\")\n",
    "    parser.add_argument(\"--pending-cost\", type=int, default=0.2,\n",
    "                        help='Pending cost for not serving a static customers in routes')\n",
    "    parser.add_argument(\"--dynamic-reward\", type=int, default=0.5,\n",
    "                        help=\"Reward for serving a Dynamic customer\")\n",
    "\n",
    "    parser.add_argument_group(\" Graph Attention models \")\n",
    "    parser.add_argument(\"--model-size\", type=int, default=64,\n",
    "                        help=\" Size of for attention models\")\n",
    "    parser.add_argument(\"--encoder-layer\", type=int, default=3,\n",
    "                        help='Number of Encoder Layers')\n",
    "    parser.add_argument(\"--num-head\", type=int, default=8,\n",
    "                        help='Number of heads in MultiHeadAttention modules')\n",
    "    parser.add_argument(\"--ff-size-actor\", type=int, default=128,\n",
    "                        help=\" Size of fully connected Feed Forward Networks\")\n",
    "    parser.add_argument(\"--ff-size-critic\", type=int, default=128,\n",
    "                        help=\" Size of fully connected Feed Forward Networks\")\n",
    "    parser.add_argument(\"--tanh-xplor\", type=int, default=10)\n",
    "    parser.add_argument(\"--edge_embedding_dim\", type=int, default=64,\n",
    "                        help = 'Edge embedding dimention for edge attributes')\n",
    "\n",
    "    # PPO Agent Training related arguments\n",
    "    parser.add_argument_group(\" Training PPO Agnet \")\n",
    "    parser.add_argument(\"--greedy\", action='store_true', default=False,\n",
    "                        help='weather to use greedy or smapling')\n",
    "    parser.add_argument(\"--learning-rate\", type=int, default=3e-4,\n",
    "                        help='Learning rate for PPO agent')\n",
    "    parser.add_argument(\"--ppo-epoch\", type=int, default=2,\n",
    "                        help='Epoch for PPO to run the sample and evaluate')\n",
    "    parser.add_argument(\"--entropy-value\", type=int, default=0.2)\n",
    "    parser.add_argument(\"--epsilon-clip\", type=int, default=0.2)\n",
    "    parser.add_argument(\"--timestep\", type=int, default=1)\n",
    "\n",
    "    parser.add_argument(\"--epoch-count\", \"-e\", type=int, default=20)\n",
    "    parser.add_argument(\"--iter-count\", \"-i\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch-size\", \"-b\", type=int, default=32)\n",
    "    parser.add_argument(\"--rate-decay\", '-d', type=float, default=0.96)\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=2)\n",
    "    parser.add_argument(\"--grad-norm-decay\", type=float, default=None)\n",
    "\n",
    "    ### Testing Related arguments\n",
    "    parser.add_argument(\"--test-batch-size\", type=int, default=8)\n",
    "\n",
    "    ### Saving paramters\n",
    "    parser.add_argument_group(\"Checkpointing\")\n",
    "    parser.add_argument(\"--output-dir\", \"-o\", type=str, default=None)\n",
    "    parser.add_argument(\"--checkpoint-period\", \"-c\", type=int, default=5)\n",
    "    parser.add_argument(\"--resume-state\", type=str, default=None)\n",
    "\n",
    "    args = parser.parse_args(argv)\n",
    "    if args.config_file is not None:\n",
    "        with open(args.config_file) as f:\n",
    "            parser.set_defaults(**json.load(f))\n",
    "\n",
    "    return parser.parse_args(argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fcb49de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Uploading data for training 640 Done\n",
      "Uploading data for testing 8 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling ORTools: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference cost on test dataset -16.82 +-  5.95\n",
      "21\n",
      "Creating Output directry... Create Output dir ./output/DVRPSR_0.025_0.5_2_230628 Initializing ADAM Optimizer ... Running PPO models \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #  1/20 :  10%|███████████▌                                                                                                        | 2/20 [00:29<04:26, 14.81s/it, l=0.8052 p=  0.01814 val=-29.86 c_val=0.2486][E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "Epoch #  1/20 :  10%|███████████▌                                                                                                        | 2/20 [00:37<05:36, 18.67s/it, l=0.8052 p=  0.01814 val=-29.86 c_val=0.2486]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'agent' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 127\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepoch_count):\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m#print('running epoch {}'.format(epoch+1))\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     train_stats\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrainppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    129\u001b[0m     agent \u001b[38;5;241m=\u001b[39m trainppo\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mold_policy\n",
      "File \u001b[0;32m~/Desktop/DVRPSR - Phase 2/TrainPPOAgent.py:101\u001b[0m, in \u001b[0;36mTrainPPOAgent.run_train\u001b[0;34m(self, args, datas, env, env_params, optim, lr_scheduler, device, epoch)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_timestep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m     u_rewards, u_losses, u_critic_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m#print(u_losses, u_critic_rewards)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DVRPSR - Phase 2/agents/AgentPPO.py:177\u001b[0m, in \u001b[0;36mAgentPPO.update\u001b[0;34m(self, memory, epoch, data, env, optim, lr_scheduler, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 177\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 147\u001b[0m\n\u001b[1;32m    143\u001b[0m         export_train_test_stats(args, start_epoch, train_stats, test_stats)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParseArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 141\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    138\u001b[0m             save_checkpoint(args, epoch, agent, optim, lr_scheduler)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     save_checkpoint(args, epoch, \u001b[43magent\u001b[49m, optim, lr_scheduler)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     export_train_test_stats(args, start_epoch, train_stats, test_stats)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'agent' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from nets import *\n",
    "from agents import *\n",
    "from problems import *\n",
    "from utils import *\n",
    "from TrainPPOAgent import *\n",
    "from utils.config import *\n",
    "from utils.ortool import *\n",
    "from utils.Misc import *\n",
    "from utils.save_load import *\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import time\n",
    "import os\n",
    "from itertools import chain\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "ortool_available = True\n",
    "\n",
    "def run(args):\n",
    "    device = torch.device(\"cpu\" if torch.backends.mps.is_available() and args.gpu else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    if args.seed is not None:\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "    if args.verbose:\n",
    "        verbose_print = print\n",
    "    else:\n",
    "        def verbose_print(*args, **kwargs):\n",
    "            pass\n",
    "\n",
    "    ## load DVRPSR problem\n",
    "\n",
    "    verbose_print(\"Uploading data for training {}\".format(args.iter_count * args.batch_size), end=\" \", flush=True)\n",
    "    train_data = torch.load(\"./data/train/DVRPSR_{}_{}_{}_{}/normalized_train.pth\".format(args.Lambda,\n",
    "                                                                                     args.dod,\n",
    "                                                                                     args.vehicle_count,\n",
    "                                                                                     args.horizon))\n",
    "    verbose_print(\"Done\")\n",
    "\n",
    "    verbose_print(\"Uploading data for testing {}\".format(args.test_batch_size), end=\" \", flush=True)\n",
    "    # test data is not normalized\n",
    "    test_data = torch.load(\"./data/test/DVRPSR_{}_{}_{}_{}/unnormalized_test.pth\".format(args.Lambda,\n",
    "                                                                                         args.dod,\n",
    "                                                                                         args.vehicle_count,\n",
    "                                                                                         args.horizon))\n",
    "    verbose_print(\"Done\")\n",
    "\n",
    "    if ortool_available:\n",
    "        reference_routes = ortool_solve(test_data)\n",
    "    else:\n",
    "        reference_routes = None\n",
    "        verbose_print(\" No reference to calculate optimality gap\", end=\" \", flush=True)\n",
    "\n",
    "    test_data.normalize()\n",
    "\n",
    "    ## Defining Environemnt for DVRPSR\n",
    "    env = {\"DVRPSR\": DVRPSR_Environment}.get(args.problem)\n",
    "    env_params = [args.pending_cost,\n",
    "                  args.dynamic_reward]\n",
    "    env_test = env(test_data, None, None, None, *env_params)\n",
    "\n",
    "    if reference_routes is not None:\n",
    "        reference_costs = eval_apriori_routes(env_test, reference_routes, 100)\n",
    "        print(\"Reference cost on test dataset {:5.2f} +- {:5.2f}\".format(reference_costs.mean(),\n",
    "                                                                         reference_costs.std()))\n",
    "\n",
    "    env_test.nodes = env_test.nodes.to(device)\n",
    "    env_test.edge_attributes = env_test.edge_attributes.to(device)\n",
    "\n",
    "    ## PPO agent for DVRPSR\n",
    "    customer_feature = 4  # customer and vehicle features are fixed\n",
    "    vehicle_feature = 8\n",
    "\n",
    "    ## customer counts\n",
    "    if args.customers_count is None:\n",
    "        args.customers_count = train_data.customer_count+1\n",
    "    print(args.customers_count)\n",
    "\n",
    "    trainppo = TrainPPOAgent(customer_feature, vehicle_feature, args.customers_count, args.model_size,\n",
    "                             args.encoder_layer, args.num_head, args.ff_size_actor, args.ff_size_critic, args.tanh_xplor,\n",
    "                             args.edge_embedding_dim, args.greedy, args.learning_rate, args.ppo_epoch, args.batch_size,\n",
    "                             args.entropy_value, args.epsilon_clip, args.epoch_count, args.timestep, args.max_grad_norm)\n",
    "\n",
    "    ## Checkpoints\n",
    "    verbose_print(\"Creating Output directry...\", end=\" \", flush=True)\n",
    "    args.output_dir = \"./output/{}_{}_{}_{}_{}\".format(\n",
    "                                                        args.problem.upper(),\n",
    "                                                        args.Lambda,\n",
    "                                                        args.dod,\n",
    "                                                        args.vehicle_count,\n",
    "                                                        time.strftime(\"%y%m%d\")\n",
    "    ) if args.output_dir is None else args.output_dir\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    write_config_file(args, os.path.join(args.output_dir, \"args.json\"))\n",
    "    verbose_print(\"Create Output dir {}\".format(args.output_dir), end=\" \", flush=True)\n",
    "\n",
    "    ## Optimizer and LR Scheduler\n",
    "    verbose_print(\"Initializing ADAM Optimizer ...\", end=\" \", flush=True)\n",
    "    lr_scheduler = None\n",
    "\n",
    "    optim = Adam([{\"params\": trainppo.agent.policy.parameters(), 'lr': args.learning_rate}])\n",
    "\n",
    "    if args.rate_decay is not None:\n",
    "        lr_scheduler = LambdaLR(optim,\n",
    "                                lr_lambda=[lambda epoch: args.learning_rate * args.rate_decay ** epoch])\n",
    "\n",
    "    if args.resume_state is None:\n",
    "        start_epoch = 0\n",
    "    else:\n",
    "        start_epoch = load_checkpoint(args, trainppo.agent.old_policy, optim, lr_scheduler)\n",
    "\n",
    "    verbose_print(\"Running PPO models \")\n",
    "    train_stats = []\n",
    "    test_stats = []\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, args.epoch_count):\n",
    "\n",
    "            #print('running epoch {}'.format(epoch+1))\n",
    "            train_stats.append(trainppo.run_train(args, train_data, env, env_params, optim, lr_scheduler, device, epoch))\n",
    "\n",
    "            agent = trainppo.agent.old_policy\n",
    "\n",
    "            if reference_routes is not None:\n",
    "                test_stats.append(trainppo.test_epoch(args, env_test, agent, reference_costs))\n",
    "            if args.rate_decay is not None:\n",
    "                lr_scheduler.step()\n",
    "            if args.grad_norm_decay is not None:\n",
    "                args.max_grad_norm *= args.grad_norm_decay\n",
    "            if (epoch + 1) % args.checkpoint_period == 0:\n",
    "                save_checkpoint(args, epoch, agent, optim, lr_scheduler)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        save_checkpoint(args, epoch, agent, optim, lr_scheduler)\n",
    "    finally:\n",
    "        export_train_test_stats(args, start_epoch, train_stats, test_stats)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(ParseArguments())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc481e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c6fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ded09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96225c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966604f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80f017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb9760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab85f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57dfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7df4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc99d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54ab22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f19815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e546f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8ea14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb461ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9af065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f124b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b0e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
