{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde0673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from problems.DVRPSR_Dataset_street import DVRPSR_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"./data/validation/DVRPSR_{}_{}_{}_{}/unnormalized_val.pth\".format(0.05,\n",
    "                                                                                     0.75,\n",
    "                                                                                     2,\n",
    "                                                                                     600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4187588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 41, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.nodes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432b44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class DVRPSR_Environment:\n",
    "    vehicle_feature = 8  # vehicle coordinates(x_i,y_i), veh_time_time_budget, total_travel_time, last_customer,\n",
    "    # next(destination) customer, last rewards, next rewards\n",
    "    customer_feature = 4\n",
    "\n",
    "    # TODO: change pending cost for rewards\n",
    "\n",
    "    def __init__(self, \n",
    "                 data = None, \n",
    "                 nodes=None, \n",
    "                 edges_attributes=None,\n",
    "                 vehicle_count = 2,\n",
    "                 vehicle_speed = 1,\n",
    "                 vehicle_time_budget = 1,\n",
    "                 pending_cost=1,\n",
    "                 dynamic_reward=0.2)\n",
    "\n",
    "        self.vehicle_count = data.vehicle_count if data is not None else vehicle_count\n",
    "        self.vehicle_speed = data.vehicle_speed if data is not None else vehicle_speed\n",
    "        self.vehicle_time_budget = data.vehicle_time_budget if data is not None else vehicle_time_budget\n",
    "\n",
    "        self.nodes = data.nodes if nodes is None else nodes\n",
    "        self.edge_attributes = data.edges_attributes if edges_attributes is None else edges_attributes\n",
    "\n",
    "        self.minibatch, self.nodes_count, _ = self.nodes.size()\n",
    "        \n",
    "        self.distance_matrix = self.edge_attributes.view((self.minibatch, self.nodes_count, self.nodes_count))\n",
    "        self.edges_attributes = None\n",
    "        \n",
    "        self.pending_cost = pending_cost\n",
    "        self.dynamic_reward = dynamic_reward\n",
    "        self.budget_penalty = budget_penalty\n",
    "\n",
    "    def _update_current_vehicles(self, dest, customer_index, tau=0):\n",
    "\n",
    "        # calculate travel time\n",
    "        # TODO: 1) in real world setting we need to calculate the distance of arc\n",
    "        # If nodes i and j are directly connected by a road segment (i, j) ∈ A, then t(i,j)=t_ij;\n",
    "        # otherwise, t(i,j)=t_ik1 +t_k1k2 +...+t_knj, where k1,...,kn ∈ V are the nodes along the\n",
    "        # shortest path from node i to node j.\n",
    "        #      2) calculate stating time for each vehicle $\\tau $, currently is set to zero\n",
    "\n",
    "        # update vehicle previous and next customer id\n",
    "        self.current_vehicle[:, :, 4] = self.current_vehicle[:, :, 5]\n",
    "        self.current_vehicle[:, :, 5] = customer_index\n",
    "\n",
    "        # get the distance from current vehicle to its next destination\n",
    "        # Convert indices to integers\n",
    "        current_idx = self.current_vehicle[:, :, 4].long()\n",
    "        next_idx    = self.current_vehicle[:, :, 5].long()\n",
    "        dist = self.distance_matrix[torch.arange(self.minibatch).unsqueeze(1), current_idx, next_idx].view(self.minibatch, 1)\n",
    "        \n",
    "        # total travel time\n",
    "        tt = dist / self.vehicle_speed\n",
    "\n",
    "        # customers which are dynamicaly appeared\n",
    "        dyn_cust = (dest[:, :, 3] > 0).float()\n",
    "\n",
    "        # budget left while travelling to destination nodes\n",
    "        budget = tt + dest[:, :, 2]\n",
    "        # print(budget, tau, tt, dest[:,:,2])\n",
    "\n",
    "        # update vehicle features based on destination nodes\n",
    "        self.current_vehicle[:, :, :2] = dest[:, :, :2]\n",
    "        self.current_vehicle[:, :, 2] -= budget\n",
    "        self.current_vehicle[:, :, 3] += tt\n",
    "        self.current_vehicle[:, :, 6] = self.current_vehicle[:, :, 7]\n",
    "        self.current_vehicle[:, :, 7] = -dist\n",
    "\n",
    "        # update vehicles states\n",
    "        self.vehicles = self.vehicles.scatter(1,\n",
    "                                              self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                            self.vehicle_feature),\n",
    "                                              self.current_vehicle)\n",
    "\n",
    "        return dist, dyn_cust\n",
    "\n",
    "    def _done(self):\n",
    "        \n",
    "        \n",
    "        pending_custtomers = (self.served^True).float().sum(-1, keepdim=True)-1\n",
    "        self.vehicle_done.scatter_(1, \n",
    "                                   self.current_vehicle_index, \n",
    "                                   torch.logical_or((pending_custtomers == 0),(self.current_vehicle[:, :, 2] <= 0)))\n",
    "        self.done = bool(self.vehicle_done.all())\n",
    "\n",
    "    def _update_mask(self, customer_index):\n",
    "\n",
    "        self.new_customer = False\n",
    "        self.served.scatter_(1, customer_index, customer_index > 0)\n",
    "\n",
    "        # cost for a vehicle to go to customer and back to deport considering service duration\n",
    "        current_idx = self.current_vehicle[:, :, 4].long()\n",
    "        dist_vehicle_customer_depot = self.distance_matrix[torch.arange(self.minibatch).unsqueeze(1), current_idx, :].squeeze(1) + self.distance_matrix[:, :, 0]\n",
    "        cost = dist_vehicle_customer_depot / self.vehicle_speed\n",
    "        cost += self.nodes[:, :, 2]\n",
    "\n",
    "\n",
    "        overtime_mask = self.current_vehicle[:, :, None, 2] - cost.unsqueeze(1)\n",
    "        overtime = torch.zeros_like(self.mask).scatter_(1,\n",
    "                                                        self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                      self.nodes_count),\n",
    "                                                        overtime_mask < 0)\n",
    "\n",
    "        self.mask = self.mask | self.served[:, None, :] | overtime | self.vehicle_done[:, :, None]\n",
    "        self.mask[:, :, 0] = 0  # depot\n",
    "\n",
    "    # updating current vehicle to find the next available vehicle\n",
    "    def _update_next_vehicle(self, veh_index=None):\n",
    "\n",
    "        if veh_index is None:\n",
    "            avail = self.vehicles[:, :, 3].clone()\n",
    "            avail[self.vehicle_done] = float('inf')\n",
    "            self.current_vehicle_index = avail.argmin(1, keepdim=True)\n",
    "        else:\n",
    "            self.current_vehicle_index = veh_index\n",
    "\n",
    "        self.current_vehicle = self.vehicles.gather(1, self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                     self.vehicle_feature))\n",
    "        self.current_vehicle_mask = self.mask.gather(1, self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                      self.nodes_count))\n",
    "\n",
    "    def _update_dynamic_customers(self):\n",
    "\n",
    "        time = self.current_vehicle[:, :, 3].clone()\n",
    "\n",
    "        reveal_dyn_reqs = torch.logical_and((self.customer_mask), (self.nodes[:, :, 3] <= time))\n",
    "\n",
    "\n",
    "        if reveal_dyn_reqs.any():\n",
    "            self.new_customer = True\n",
    "            self.customer_mask = self.customer_mask ^ reveal_dyn_reqs\n",
    "            self.mask = self.mask ^ reveal_dyn_reqs[:, None, :].expand(-1, self.vehicle_count, -1)\n",
    "            self.vehicle_done = torch.logical_and(self.vehicle_done, (reveal_dyn_reqs.any(1) ^ True).unsqueeze(1))\n",
    "            self.vehicles[:, :, 3] = torch.max(self.vehicles[:, :, 3], time)\n",
    "            self._update_next_vehicle()\n",
    "\n",
    "    def reset(self):\n",
    "        # reset vehicle (minibatch*veh_count*veh_feature)\n",
    "        self.vehicles = self.nodes.new_zeros((self.minibatch, self.vehicle_count, self.vehicle_feature))\n",
    "        self.vehicles[:, :, :2] = self.nodes[:, :1, :2]\n",
    "        self.vehicles[:, :, 2] = self.vehicle_time_budget\n",
    "\n",
    "        # reset vehicle done\n",
    "        self.vehicle_done = self.nodes.new_zeros((self.minibatch, self.vehicle_count), dtype=torch.bool)\n",
    "        self.done = False\n",
    "        \n",
    "        # initialize reward as tour length\n",
    "        self.tour_length = torch.zeros((self.minibatch,1)).to(self.nodes.device)\n",
    "        \n",
    "        # reset cust_mask\n",
    "        self.customer_mask = self.nodes[:, :, 3] > 0\n",
    "\n",
    "        # reset new customers and served customer since now to zero (all false)\n",
    "        self.new_customer = True\n",
    "        self.served = torch.zeros_like(self.customer_mask)\n",
    "\n",
    "        # reset mask (minibatch*veh_count*nodes)\n",
    "        self.mask = self.customer_mask[:, None, :].repeat(1, self.vehicle_count, 1)\n",
    "\n",
    "        # reset current vehicle index, current vehicle, current vehicle mask\n",
    "        self.current_vehicle_index = self.nodes.new_zeros((self.minibatch, 1), dtype=torch.int64)\n",
    "\n",
    "        self.current_vehicle = self.vehicles.gather(1,\n",
    "                                                    self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                  self.vehicle_feature))\n",
    "        self.current_vehicle_mask = self.mask.gather(1,\n",
    "                                                     self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                   self.nodes_count))\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.done:\n",
    "            # penalty for pending customers\n",
    "            pending_customers = (self.served ^ True).float().sum(-1, keepdim=True) - 1\n",
    "            self.tour_length -= pending_customers*self.pending_cost\n",
    "        return self.tour_length\n",
    "        \n",
    "    \n",
    "    def step(self, customer_index, veh_index=None):\n",
    "        dest = self.nodes.gather(1, customer_index[:, :, None].expand(-1, -1, self.customer_feature))\n",
    "        dist, dyn_cust = self._update_current_vehicles(dest, customer_index)\n",
    "\n",
    "        self._done(customer_index)\n",
    "        self._update_mask(customer_index)\n",
    "        self._update_next_vehicle(veh_index)\n",
    "        self.tour_length -= dist \n",
    "        self._update_dynamic_customers()\n",
    "\n",
    "\n",
    "    def state_dict(self, dest_dict=None):\n",
    "        if dest_dict is None:\n",
    "            dest_dict = {'vehicles': self.vehicles,\n",
    "                         'vehicle_done': self.vehicle_done,\n",
    "                         'served': self.served,\n",
    "                         'mask': self.mask,\n",
    "                         'current_vehicle_index': self.current_vehicle_index}\n",
    "\n",
    "        else:\n",
    "            dest_dict[\"vehicles\"].copy_(self.vehicles)\n",
    "            dest_dict[\"vehicle_done\"].copy_(self.vehicle_done)\n",
    "            dest_dict[\"served\"].copy_(self.served)\n",
    "            dest_dict[\"mask\"].copy_(self.mask)\n",
    "            dest_dict[\"current_vehicle_index\"].copy_(self.current_vehicle_index)\n",
    "\n",
    "        return dest_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.vehicles.copy_(state_dict[\"vehicles\"])\n",
    "        self.vehicle_done.copy_(state_dict[\"vehicle_done\"])\n",
    "        self.served.copy_(state_dict[\"served\"])\n",
    "        self.mask.copy_(state_dict[\"mask\"])\n",
    "        self.current_vehicle_index.copy_(state_dict[\"current_vehicle_index\"])\n",
    "\n",
    "        self.current_vehicle = self.vehicles.gather(1,\n",
    "                                                    self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                  self.vehicle_feature))\n",
    "        self.current_vehicle_mask = self.mask.gather(1, self.current_vehicle_index[:, :, None].expand(-1, -1,\n",
    "                                                                                                      self.customer_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79ce420",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DVRPSR_Environment(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e7ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96690c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(torch.tensor([[3],[2],[3],[3],[2],[3],[3],[2],[3],[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36a02f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.],\n",
       "        [39.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(env.served^True).float().sum(-1, keepdim=True)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb9c5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, query_size, key_size=None, value_size=None, edge_dim_size=None, bias=False):\n",
    "\n",
    "        super(GraphMultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.query_size = query_size\n",
    "\n",
    "        self.key_size = self.query_size if key_size is None else key_size\n",
    "        self.value_size = self.key_size if value_size is None else value_size\n",
    "        self.edge_dim_size = self.query_size // 2 if edge_dim_size is None else edge_dim_size\n",
    "\n",
    "        self.scaling_factor = self.key_size ** -0.5\n",
    "\n",
    "        self.keys_per_head = self.key_size // self.num_head\n",
    "        self.values_per_head = self.value_size // self.num_head\n",
    "        self.edge_size_per_head = self.edge_dim_size // self.num_head\n",
    "\n",
    "        self.edge_embedding = nn.Linear(self.edge_dim_size, self.num_head * self.edge_size_per_head, bias=bias)\n",
    "        self.query_embedding = nn.Linear(self.query_size, self.num_head * self.keys_per_head, bias=bias)\n",
    "        self.key_embedding = nn.Linear(self.key_size, self.num_head * self.keys_per_head, bias=bias)\n",
    "        self.value_embedding = nn.Linear(self.value_size, self.num_head * self.values_per_head, bias=bias)\n",
    "        self.recombine = nn.Linear(self.num_head * self.values_per_head, self.value_size, bias=bias)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.query_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.key_embedding.weight)\n",
    "        inv_sq_dv = self.value_size ** -0.5\n",
    "        nn.init.uniform_(self.value_embedding.weight, -inv_sq_dv, inv_sq_dv)\n",
    "        \n",
    "        \n",
    "    def precompute(self, keys, values=None):\n",
    "\n",
    "        values = keys if values is None else values\n",
    "\n",
    "        size_KV = keys.size(-2)\n",
    "\n",
    "        self.K_project_pre = self.key_embedding(keys).view(\n",
    "            -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "\n",
    "        self.V_project_pre = self.value_embedding(values).view(\n",
    "            -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, queries, keys=None, values=None, edge_attributes=None, mask=None, edge_mask=None):\n",
    "\n",
    "        *batch_size, size_Q, _ = queries.size()\n",
    "\n",
    "        # get queries projection\n",
    "        Q_project = self.query_embedding(queries).view(\n",
    "            -1, size_Q, self.num_head, self.keys_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get keys projection\n",
    "        if keys is None:\n",
    "            if self.K_project_pre is None:\n",
    "                size_KV = size_Q\n",
    "                K_project = self.key_embedding(queries).view(\n",
    "                    -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "            else:\n",
    "                size_KV = self.K_project_pre.size(-1)\n",
    "                K_project = self.K_project_pre\n",
    "        else:\n",
    "            size_KV = keys.size(-2)\n",
    "            K_project = self.key_embedding(keys).view(\n",
    "                -1, size_KV, self.num_head, self.keys_per_head).permute(0, 2, 3, 1)\n",
    "\n",
    "        # get values projection\n",
    "        if values is None:\n",
    "            if self.V_project_pre is None:\n",
    "                V_project = self.value_embedding(queries).view(\n",
    "                    -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "            else:\n",
    "                V_project = self.V_project_pre\n",
    "        else:\n",
    "            V_project = self.value_embedding(values).view(\n",
    "                -1, size_KV, self.num_head, self.values_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # calculate the compability\n",
    "        attention = Q_project.matmul(K_project)\n",
    "        attention *= self.scaling_factor\n",
    "\n",
    "        # if edge attributes are required\n",
    "        if edge_attributes is not None:\n",
    "            # TODO: edge mask (is it required)\n",
    "            edge_project = self.edge_embedding(edge_attributes).view(\n",
    "                -1, size_Q, size_Q, self.num_head, self.edge_size_per_head)\n",
    "            edge_project_expanded = edge_project.mean(-1).permute(0, 3, 1, 2)\n",
    "\n",
    "            attention = attention * edge_project_expanded\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            if mask.numel() * self.num_head == attention.numel():\n",
    "                m = mask.view(-1, 1, size_Q, size_KV).expand_as(attention)\n",
    "            else:\n",
    "                m = mask.view(-1, 1, 1, size_KV).expand_as(attention)\n",
    "\n",
    "            attention[m.bool()] = -float('inf')\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = attention.matmul(V_project).permute(0, 2, 1, 3).contiguous().view(\n",
    "            *batch_size, size_Q, self.num_head * self.values_per_head)\n",
    "\n",
    "        output = self.recombine(attention)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nets import GraphMultiHeadAttention\n",
    "\n",
    "class GraphEncoderlayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, model_size, ff_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = GraphMultiHeadAttention(num_head, query_size=model_size)\n",
    "        self.BN1 = nn.BatchNorm1d(model_size)\n",
    "        self.FFN_layer1 = nn.Linear(model_size, ff_size)\n",
    "\n",
    "        self.FFN_layer2 = nn.Linear(ff_size, model_size)\n",
    "        self.BN2 = nn.BatchNorm1d(model_size)\n",
    "\n",
    "    def forward(self, h, e=None, mask=None):\n",
    "        h_attn = self.attention(h, edge_attributes=e, mask=mask)\n",
    "        h_bn = self.BN1((h_attn + h).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        h_layer1 = F.relu(self.FFN_layer1(h_bn))\n",
    "        h_layer2 = self.FFN_layer2(h_layer1)\n",
    "\n",
    "        h_out = self.BN2((h_bn + h_layer2).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        if mask is not None:\n",
    "            h_out[mask] = 0\n",
    "\n",
    "        return h_out\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_head, model_size, ff_size):\n",
    "        super().__init__()\n",
    "\n",
    "        for l in range(encoder_layer):\n",
    "            self.add_module(str(l), GraphEncoderlayer(num_head, model_size, ff_size))\n",
    "\n",
    "    def forward(self, h_in, e_in=None, mask=None):\n",
    "\n",
    "        h = h_in\n",
    "        e = e_in\n",
    "\n",
    "        for child in self.children():\n",
    "            h = child(h, e, mask=mask)\n",
    "        return h\n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from nets import GraphMultiHeadAttention\n",
    "from nets.Encoder import GraphEncoder\n",
    "\n",
    "class GraphAttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, customer_feature, vehicle_feature, model_size=128, encoder_layer=3,\n",
    "                 num_head=8, ff_size=128, tanh_xplor=10, edge_embedding_dim=64, greedy=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # get models parameters for encoding-decoding\n",
    "        self.model_size = model_size\n",
    "        self.scaling_factor = self.model_size ** 0.5\n",
    "        self.tanh_xplor = tanh_xplor\n",
    "        self.greedy = greedy\n",
    "\n",
    "        # Initialize encoder and embeddings\n",
    "        self.customer_encoder = GraphEncoder(encoder_layer=3, num_head=8, model_size=model_size, ff_size=ff_size)\n",
    "        self.customer_embedding = nn.Linear(customer_feature, model_size)\n",
    "        self.depot_embedding = nn.Linear(customer_feature, model_size)\n",
    "\n",
    "        # initialize edge embedding\n",
    "        self.edge_embedding = nn.Linear(1, edge_embedding_dim)\n",
    "\n",
    "        # Initialize vehicle embedding and encoding\n",
    "        # self.vehicle_embedding = nn.Linear(vehicle_feature, ff_size, bias=False)\n",
    "\n",
    "        self.fleet_attention = GraphMultiHeadAttention(num_head, vehicle_feature, model_size)\n",
    "\n",
    "        self.vehicle_attention = GraphMultiHeadAttention(num_head, model_size)\n",
    "\n",
    "        # customer projection\n",
    "        self.customer_projection = nn.Linear(self.model_size, self.model_size)  # TODO: MLP instaed of nn.Linear\n",
    "\n",
    "    def encode_customers(self, env, customer_mask=None):\n",
    "\n",
    "        customer_emb = torch.cat((self.depot_embedding(env.nodes[:, :1, :]),\n",
    "                                  self.customer_embedding(env.nodes[:, 1:, :])), dim=1)\n",
    "        if customer_mask is not None:\n",
    "            customer_emb[customer_mask] = 0\n",
    "\n",
    "        edge_emb = self.edge_embedding(env.edge_attributes)\n",
    "\n",
    "        self.customer_encoding = self.customer_encoder(customer_emb, edge_emb, mask=customer_mask)\n",
    "\n",
    "        self.fleet_attention.precompute(self.customer_encoding)\n",
    "\n",
    "        self.customer_representation = self.customer_projection(self.customer_encoding)\n",
    "        if customer_mask is not None:\n",
    "            self.customer_representation[customer_mask] = 0\n",
    "\n",
    "    def vehicle_representation(self, vehicles, vehicle_index, vehicle_mask=None):\n",
    "\n",
    "        fleet_representation = self.fleet_attention(vehicles, mask=vehicle_mask)\n",
    "        vehicle_query = fleet_representation.gather(0, vehicle_index.unsqueeze(2).expand(-1, -1, self.model_size))\n",
    "\n",
    "        return self.vehicle_attention(vehicle_query, fleet_representation, fleet_representation)\n",
    "    \n",
    "\n",
    "    def score_customers(self, vehicle_representation):\n",
    "\n",
    "        # print(vehicle_representation.size(), self.customer_representation.size())\n",
    "        compact = torch.bmm(vehicle_representation,\n",
    "                            self.customer_representation.transpose(2, 1))\n",
    "        compact *= self.scaling_factor\n",
    "\n",
    "        if self.tanh_xplor is not None:\n",
    "            compact = self.tanh_xplor * compact.tanh()\n",
    "\n",
    "        return compact\n",
    "\n",
    "    def get_prop(self, compact, vehicle_mask=None):\n",
    "        if vehicle_mask is not None:\n",
    "            compact = compact.masked_fill(vehicle_mask.unsqueeze(-1), float('-inf'))\n",
    "        compact = F.softmax(compact, dim=-1)\n",
    "        return compact\n",
    "\n",
    "\n",
    "    def step(self, env, old_action=None):\n",
    "\n",
    "        _vehicle_representation = self.vehicle_representation(env.vehicles,\n",
    "                                                              env.current_vehicle_index,\n",
    "                                                              env.current_vehicle_mask)\n",
    "\n",
    "        compact = self.score_customers(_vehicle_representation)\n",
    "        prop = self.get_prop(compact, env.current_vehicle_mask)\n",
    "        # print(compact.size())\n",
    "\n",
    "        # step actions based on model act or evalaute\n",
    "        if old_action is not None:\n",
    "\n",
    "            # get entropy\n",
    "            dist = Categorical(prop)\n",
    "            old_actions_logp = dist.log_prob(old_action[:, 1].unsqueeze(-1))\n",
    "            entropy = dist.entropy()\n",
    "\n",
    "            is_done = float(env.done)\n",
    "\n",
    "            entropy *= (1. - is_done)\n",
    "            old_actions_logp *= (1. - is_done)\n",
    "            return old_action[:, 1].unsqueeze(-1), entropy, old_actions_logp\n",
    "\n",
    "\n",
    "        else:\n",
    "            dist = Categorical(prop)\n",
    "\n",
    "            if self.greedy:\n",
    "                _, customer_index = p.max(dim=-1)\n",
    "            else:\n",
    "                customer_index = dist.sample()\n",
    "\n",
    "            is_done = float(env.done)\n",
    "\n",
    "            logp = dist.log_prob(customer_index)\n",
    "            logp *= (1. - is_done)\n",
    "\n",
    "            return customer_index, logp\n",
    "\n",
    "    def forward(self, env, old_actions=None, is_update=False):\n",
    "\n",
    "        if is_update:\n",
    "            env.reset()\n",
    "            entropys, old_actions_logps = [], []\n",
    "\n",
    "            steps = old_actions.size(0)\n",
    "\n",
    "            for i in range(steps):\n",
    "                if env.new_customer:\n",
    "                    self.encode_customers(env, env.customer_mask)\n",
    "\n",
    "                old_action = old_actions[i, :, :]\n",
    "                next_action = old_actions[i + 1, :, :] if i < steps - 1 else old_action\n",
    "\n",
    "\n",
    "                next_vehicle_index = next_action[:, 0].unsqueeze(-1)\n",
    "                # print(next_vehicle_index)\n",
    "\n",
    "                customer_index, entropy, logp = self.step(env, old_action)\n",
    "\n",
    "                env.step(customer_index, next_vehicle_index)\n",
    "\n",
    "                old_actions_logps.append(logp)\n",
    "                entropys.append(entropy)\n",
    "\n",
    "            entropys = torch.cat(entropys, dim=1)\n",
    "            num_e = entropys.ne(0).float().sum(1)\n",
    "            entropy = entropys.sum(1) / num_e\n",
    "\n",
    "            old_actions_logps = torch.cat(old_actions_logps, dim=1)\n",
    "            old_actions_logps = old_actions_logps.sum(1)\n",
    "\n",
    "            return entropy, old_actions_logps, 0\n",
    "\n",
    "        else:\n",
    "            env.reset()\n",
    "            actions, logps, rewards = [], [], []\n",
    "\n",
    "            while not env.done:\n",
    "                if env.new_customer:\n",
    "                    self.encode_customers(env, env.customer_mask)\n",
    "\n",
    "                customer_index, logp = self.step(env)\n",
    "                actions.append((env.current_vehicle_index, customer_index))\n",
    "                logps.append(logp)\n",
    "                rewards.append(env.step(customer_index))\n",
    "\n",
    "            # actions = torch.cat(actions, dim=1)\n",
    "            logps = torch.cat(logps, dim=1)\n",
    "            logp = logps.sum(dim=1)\n",
    "\n",
    "            rewards = torch.cat(rewards, dim=1)\n",
    "            rewards = rewards.sum(dim=1)\n",
    "\n",
    "            return actions, logp, rewards\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed63a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    # critic will take environment as imput and ouput the values for loss function\n",
    "    # which is basically the estimation of complexity of actions\n",
    "\n",
    "    def __init__(self, model, customers_count, ff_size=512):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.ff_layer1 = nn.Linear(customers_count, ff_size)\n",
    "        self.ff_layer2 = nn.Linear(ff_size, customers_count)\n",
    "\n",
    "    def eval_step(self, env, compatibility, customer_index):\n",
    "        compact = compatibility.clone()\n",
    "        compact[env.current_vehicle_mask] = 0\n",
    "\n",
    "        value = self.ff_layer1(compact)\n",
    "        value = F.relu(value)\n",
    "        value = self.ff_layer2(value)\n",
    "\n",
    "        val = value.gather(2, customer_index.unsqueeze(1)).expand(-1, 1, -1)\n",
    "        return val.squeeze(1)\n",
    "\n",
    "    def __call__(self, env):\n",
    "        self.model.encode_customers(env)\n",
    "        env.reset()\n",
    "\n",
    "        values = []\n",
    "\n",
    "        while not env.done:\n",
    "            _vehicle_presentation = self.model.vehicle_representation(env.vehicles,\n",
    "                                                                      env.current_vehicle_index,\n",
    "                                                                      env.current_vehicle_mask)\n",
    "            compatibility = self.model.score_customers(_vehicle_presentation)\n",
    "            prop = self.model.get_prop(compatibility, env.current_vehicle_mask)\n",
    "            dist = Categorical(prop)\n",
    "            customer_index = dist.sample()\n",
    "\n",
    "            values.append(self.eval_step(env, compatibility, customer_index))\n",
    "\n",
    "        return values[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a035bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nets import GraphAttentionModel\n",
    "from agents.Critic import Critic\n",
    "\n",
    "\n",
    "class Actor_Critic(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 customer_feature,\n",
    "                 vehicle_feature,\n",
    "                 customers_count,\n",
    "                 model_size=128,\n",
    "                 encoder_layer=3,\n",
    "                 num_head=8,\n",
    "                 ff_size_actor=128,\n",
    "                 ff_size_critic=512,\n",
    "                 tanh_xplor=10,\n",
    "                 edge_embedding_dim=64,\n",
    "                 greedy=False):\n",
    "        super(Actor_Critic, self).__init__()\n",
    "\n",
    "        model = GraphAttentionModel(customer_feature, vehicle_feature, model_size, encoder_layer,\n",
    "                                    num_head, ff_size_actor, tanh_xplor, edge_embedding_dim, greedy)\n",
    "        self.actor = model\n",
    "\n",
    "        self.critic = Critic(model, customers_count, ff_size_critic)\n",
    "\n",
    "    def act(self, env, old_actions=None, is_update=False):\n",
    "        actions, logps, rewards = self.actor(env)\n",
    "        return actions, logps, rewards\n",
    "\n",
    "    def evaluate(self, env, old_actions, is_update):\n",
    "        values = self.critic(env)\n",
    "        entropys, old_logps, _ = self.actor(env, old_actions, is_update)\n",
    "        return entropys, old_logps, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1be153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from agents.Actor_Critic import Actor_Critic\n",
    "from problems import DVRPSR_Environment\n",
    "\n",
    "\n",
    "class AgentPPO:\n",
    "\n",
    "    def __init__(self,\n",
    "                 customer_feature,\n",
    "                 vehicle_feature,\n",
    "                 customers_count,\n",
    "                 model_size=128,\n",
    "                 encoder_layer=3,\n",
    "                 num_head=8,\n",
    "                 ff_size_actor=128,\n",
    "                 ff_size_critic=128,\n",
    "                 tanh_xplor=10,\n",
    "                 edge_embedding_dim=64,\n",
    "                 greedy=False,\n",
    "                 learning_rate=3e-4,\n",
    "                 ppo_epoch=3,\n",
    "                 batch_size=128,\n",
    "                 entropy_value=0.2,\n",
    "                 epsilon_clip=0.2,\n",
    "                 max_grad_norm = 2):\n",
    "\n",
    "        self.policy = Actor_Critic(customer_feature, vehicle_feature, customers_count, model_size,\n",
    "                                   encoder_layer, num_head, ff_size_actor, ff_size_critic,\n",
    "                                   tanh_xplor, edge_embedding_dim, greedy)\n",
    "\n",
    "        self.old_policy = Actor_Critic(customer_feature, vehicle_feature, customers_count, model_size,\n",
    "                                       encoder_layer, num_head, ff_size_actor, ff_size_critic,\n",
    "                                       tanh_xplor, edge_embedding_dim, greedy)\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # ppo update parameters\n",
    "        # self.learning_rate = learning_rate\n",
    "        self.ppo_epoch = ppo_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_value = entropy_value\n",
    "        self.epsilon_clip = epsilon_clip\n",
    "        self.batch_index = 1\n",
    "\n",
    "        # initialize the Adam optimizer\n",
    "        #self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "\n",
    "        # actor-critic parameters\n",
    "        self.customer_feature = customer_feature\n",
    "        self.vehicle_feature = vehicle_feature\n",
    "        self.customers_count = customers_count\n",
    "        self.model_size = model_size\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.num_head = num_head\n",
    "        self.ff_size_actor = ff_size_actor\n",
    "        self.ff_size_critic = ff_size_critic\n",
    "        self.tanh_xplor = tanh_xplor\n",
    "        self.edge_embedding_dim = edge_embedding_dim\n",
    "        self.greedy = greedy\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.times, self.losses, self.rewards, self.critic_rewards = [], [], [], []\n",
    "\n",
    "    def advantage_normalization(self, advantage):\n",
    "\n",
    "        std = advantage.std()\n",
    "\n",
    "        assert std != 0. and not torch.isnan(std), 'Need nonzero std'\n",
    "\n",
    "        norm_advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        return norm_advantage\n",
    "\n",
    "    def pad_actions(self, actions):\n",
    "        max_len = max(a.size(0) for a in actions)\n",
    "        padded_actions = torch.zeros(len(actions), max_len, actions[0].size(1), dtype=actions[0].dtype)\n",
    "        for i, a in enumerate(actions):\n",
    "            length = a.size(0)\n",
    "            padded_actions[i, :length] = a\n",
    "        return padded_actions\n",
    "\n",
    "    def update(self, memory, epoch, data=None, env=None, optim=None, lr_scheduler=None, device=None):\n",
    "\n",
    "        old_nodes = torch.stack(memory.nodes)\n",
    "        old_edge_attributes = torch.stack(memory.edge_attributes)\n",
    "        old_rewards = torch.stack(memory.rewards).unsqueeze(-1)\n",
    "        old_log_probs = torch.stack(memory.log_probs).unsqueeze(-1)\n",
    "        padded_actions = torch.stack(memory.actions)\n",
    "\n",
    "        # preprocessing on old actions\n",
    "        #padded_actions, max_length = self.pad_actions(memory.actions)\n",
    "\n",
    "        # create update data for PPO\n",
    "        datas = []\n",
    "\n",
    "        # print(memory.actions.size())\n",
    "\n",
    "        # Create update data for PPO\n",
    "        datas = [Data(nodes=nodes, \n",
    "                      edge_attributes=edge_attributes, \n",
    "                      actions=actions, \n",
    "                      rewards=rewards, \n",
    "                      log_probs=log_probs)\n",
    "                 for nodes, edge_attributes, actions, rewards, log_probs in zip(old_nodes, old_edge_attributes,\n",
    "                                                                                padded_actions, old_rewards,\n",
    "                                                                                old_log_probs)]\n",
    "        # print(datas[0], self.batch_size)\n",
    "\n",
    "        self.policy.to(device)\n",
    "        self.policy.train()\n",
    "\n",
    "        data_loader = DataLoader(datas, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # scheduler = LambdaLR(self.optimizer, lr_lambda=lambda f: 0.96 ** epoch)\n",
    "        value_buffer = 0\n",
    "\n",
    "        env = env if env is not None else DVRPSR_Environment\n",
    "\n",
    "        for i in range(self.ppo_epoch):\n",
    "\n",
    "            \n",
    "            epoch_start = time.time()\n",
    "            start = epoch_start\n",
    "\n",
    "            self.times, self.losses, self.rewards, self.critic_rewards = [], [], [], []\n",
    "\n",
    "            for batch_index, minibatch_data in enumerate(data_loader):\n",
    "\n",
    "                self.batch_index += 1\n",
    "                #minibatch_data =  minibatch_data.to(device)\n",
    "\n",
    "                if data.customer_mask is None:\n",
    "                    nodes = minibatch_data.nodes.to(device)\n",
    "                    customer_mask = None\n",
    "                    edge_attributes = minibatch_data.edge_attributes.to(device)\n",
    "\n",
    "                nodes = nodes.view(self.batch_size, self.customers_count, self.customer_feature)\n",
    "                edge_attributes = edge_attributes.view(self.batch_size, self.customers_count * self.customers_count, 1)\n",
    "\n",
    "                old_actions_for_env = minibatch_data.actions.view(self.batch_size, max_length, 2).permute(1, 0, 2).to(device)\n",
    "                #print(old_actions_for_env)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dyna_env = env(data, nodes, customer_mask, edge_attributes)\n",
    "                    entropy, log_probs, values = self.policy.evaluate(dyna_env, old_actions_for_env, True)\n",
    "\n",
    "                # normalize the rewards and get the MSE loss with critics values\n",
    "                R = minibatch_data.rewards\n",
    "                R_norm = self.advantage_normalization(R)\n",
    "\n",
    "                mse_loss = self.MSE_loss(R_norm, values.squeeze(-1))\n",
    "\n",
    "                # PPO ration (r(0)_t)\n",
    "                ratio = torch.exp(log_probs - minibatch_data.log_probs)\n",
    "\n",
    "                # PPO advantage\n",
    "                advantage = R_norm - values.detach()\n",
    "\n",
    "                # PPO overall loss function\n",
    "                actor_loss1 = ratio * advantage\n",
    "                actor_loss2 = torch.clamp(ratio, 1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantage\n",
    "\n",
    "                actor_loss = torch.min(actor_loss1, actor_loss2)\n",
    "\n",
    "                # total loss\n",
    "                loss = actor_loss + 0.5 * mse_loss - self.entropy_value * entropy\n",
    "\n",
    "                # optimizer and backpropogation\n",
    "                #self.optimizer.zero_grad()\n",
    "                optim.zero_grad()\n",
    "                loss.mean().backward(retain_graph = True)\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                \n",
    "                #self.optimizer.step()\n",
    "                optim.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                self.rewards.append(torch.mean(R_norm.detach()).item())\n",
    "                self.losses.append(torch.mean(loss.detach()).item())\n",
    "                self.critic_rewards.append(torch.mean(values.detach()).item())\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        return self.rewards, self.losses, self.critic_rewards\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from agents import AgentPPO\n",
    "from utils import Memory\n",
    "from utils.Misc import formate_old_actions\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TrainPPOAgent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 customer_feature,\n",
    "                 vehicle_feature,\n",
    "                 customers_count,\n",
    "                 model_size=128,\n",
    "                 encoder_layer=3,\n",
    "                 num_head=8,\n",
    "                 ff_size_actor=128,\n",
    "                 ff_size_critic=128,\n",
    "                 tanh_xplor=10,\n",
    "                 edge_embedding_dim=64,\n",
    "                 greedy=False,\n",
    "                 learning_rate=3e-4,\n",
    "                 ppo_epoch=3,\n",
    "                 batch_size=128,\n",
    "                 entropy_value=0.2,\n",
    "                 epsilon_clip=0.2,\n",
    "                 epoch=50,\n",
    "                 timestep=2,\n",
    "                 max_grad_norm=2):\n",
    "\n",
    "        self.greedy = greedy\n",
    "        self.memory = Memory()\n",
    "        self.batch_size = batch_size\n",
    "        self.customers_count = customers_count\n",
    "        self.update_timestep = timestep\n",
    "        self.epoch = epoch\n",
    "        self.agent = AgentPPO(customer_feature, vehicle_feature, customers_count, model_size,\n",
    "                              encoder_layer, num_head, ff_size_actor, ff_size_critic,\n",
    "                              tanh_xplor, edge_embedding_dim, greedy, learning_rate,\n",
    "                              ppo_epoch, batch_size, entropy_value, epsilon_clip, max_grad_norm)\n",
    "\n",
    "    def run_train(self, args, datas, env, env_params, optim, lr_scheduler, device, epoch):\n",
    "\n",
    "        train_data_loader = DataLoader(datas, batch_size=self.batch_size, shuffle=True)\n",
    "        #print(self.batch_size)\n",
    "\n",
    "        memory = Memory()\n",
    "        self.agent.old_policy.to(device)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_prop = 0\n",
    "        epoch_val = 0\n",
    "        epoch_c_val = 0\n",
    "\n",
    "        self.agent.old_policy.train()\n",
    "        times, losses, rewards1, critic_rewards = [], [], [], []\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        start_time = epoch_start\n",
    "\n",
    "        with tqdm(train_data_loader, desc=\"Epoch #{: >3d}/{: <3d}\".format(epoch + 1, args.epoch_count)) as progress:\n",
    "\n",
    "            for batch_index, minibatch in enumerate(progress):\n",
    "\n",
    "                if datas.customer_mask is None:\n",
    "                    nodes, customer_mask, edge_attributes = minibatch[0].to(device), None, minibatch[1].to(device)\n",
    "\n",
    "                nodes = nodes.view(self.batch_size, self.customers_count, 4)\n",
    "                edge_attributes = edge_attributes.view(self.batch_size, self.customers_count * self.customers_count, 1)\n",
    "\n",
    "                print(datas.nodes.device, nodes.device, edge_attributes.device)\n",
    "\n",
    "                dyna_env = env(datas=None, nodes, customer_mask, edge_attributes, *env_params)\n",
    "\n",
    "                actions, logps, rewards = self.agent.old_policy.act(dyna_env)\n",
    "\n",
    "                actions = actions.to(torch.device('cpu')).detach()\n",
    "                logps = logps.to(torch.device('cpu')).detach()\n",
    "                rewards = rewards.to(torch.device('cpu')).detach()\n",
    "                \n",
    "                ## formate the actions for memory\n",
    "                actions = formate_old_actions(actions)\n",
    "                actions = torch.tensor(actions)\n",
    "                actions = actions.permute(1, 0, 2)\n",
    "\n",
    "                #print(actions.size())\n",
    "\n",
    "                memory.nodes.extend(minibath[0])\n",
    "                memory.edge_attributes.extend(minibatch[1])\n",
    "                memory.rewards.extend(rewards)\n",
    "                memory.log_probs.extend(logps)\n",
    "                memory.actions.extend(actions)\n",
    "\n",
    "                if (batch_index + 1) % self.update_timestep == 0:\n",
    "                    u_rewards, u_losses, u_critic_rewards = self.agent.update(memory, epoch, datas, env, optim, lr_scheduler, device)\n",
    "                    #print(u_losses, u_critic_rewards)\n",
    "                    memory.clear()\n",
    "\n",
    "                prob = torch.stack([logps]).sum(dim=0).exp().mean()\n",
    "                val = torch.stack([rewards]).sum(dim=0).mean()\n",
    "                c_val = torch.tensor(u_critic_rewards).mean()\n",
    "                u_losses = torch.tensor(u_losses).mean()\n",
    "\n",
    "                progress.set_postfix_str(\"l={:.4g} p={:9.4g} val={:6.4g} c_val={:6.4g}\".format(\n",
    "                    u_losses.item(), prob.item(), val.item(), c_val.item()))\n",
    "\n",
    "                epoch_loss += u_losses.item()\n",
    "                epoch_prop += prob.item()\n",
    "                epoch_val += val.item()\n",
    "                epoch_c_val += c_val.item()\n",
    "\n",
    "            return tuple(stats / args.iter_count for stats in (epoch_loss, epoch_prop, epoch_val, epoch_c_val))\n",
    "\n",
    "    def test_epoch(self, args, env, agent, ref_costs):\n",
    "        agent.eval()\n",
    "        costs = env.nodes.new_zeros(env.minibatch)\n",
    "\n",
    "        for _ in range(100):\n",
    "            _, _, rewards = agent.act(env)\n",
    "            costs += torch.stack([rewards]).sum(dim=0).squeeze(-1)\n",
    "\n",
    "        costs = costs / 100\n",
    "\n",
    "        mean = costs.mean()\n",
    "        std = costs.std()\n",
    "        gap = (costs.to(ref_costs.device) / ref_costs - 1).mean()\n",
    "\n",
    "        print(\"Cost on test dataset: {:5.2f} +- {:5.2f} ({:.2%})\".format(mean, std, gap))\n",
    "        return mean.item(), std.item(), gap.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb4be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2948e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3108d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0bf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8629dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624ee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176dd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd449ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
